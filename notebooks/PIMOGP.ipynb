{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb176c7c",
   "metadata": {},
   "source": [
    "# Permutation invariant Multi-output GPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cde787",
   "metadata": {},
   "source": [
    "Now on to a complete application of all the individual pieces. That is, \n",
    "1. an LMC model, \n",
    "2. with an explicit covariance function over the outputs, and \n",
    "3. where each latent function is invariant to a permutation $\\sigma(\\{1,\\ldots,p\\})$ of its inputs.\n",
    "\n",
    "I'll set this up like in the previous notebooks where we assume two groups in the LMC, with a suitably truncated eigen-decomp of the output covariance in each case. The first group will have a fairly large length scale, enforcing smoothly varying functions, while the second will have a short lengthscale to capture larger fluctuations. Each latent function will be invariant to a specific permutation of the inputs, and for simplicity I'll assume the functions have 4 inputs, and each latent function is invariant like so: $u(x_1,x_2,x_3,x_4)=u(x_2,x_1,x_3,x_4)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5413c4",
   "metadata": {},
   "source": [
    "## Setting up simulation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a263d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Code starts as before, with some imports and generating data\n",
    "# Some imports\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import linear_operator\n",
    "r = 2 # Number of latents\n",
    "# Easiest way to sample from a GP using gpytorch?\n",
    "# Setting up the grid\n",
    "n = 11\n",
    "x1 = torch.linspace(0,1,n)\n",
    "x2 = torch.linspace(0,1,n)\n",
    "x3 = torch.linspace(0,1,n)\n",
    "x4 = torch.linspace(0,1,n)\n",
    "x1_grid, x2_grid, x3_grid, x4_grid = torch.meshgrid(x1,x2,x3,x4,indexing='ij')\n",
    "X1 = x1_grid.reshape(-1)\n",
    "X2 = x2_grid.reshape(-1)\n",
    "X3 = x3_grid.reshape(-1)\n",
    "X4 = x4_grid.reshape(-1)\n",
    "X = torch.stack([X1,X2,X3,X4],dim=-1)\n",
    "X_flipped = torch.stack([X2,X1,X3,X4],dim=-1)\n",
    "\n",
    "u1 = torch.zeros(X.size(dim=0),r)\n",
    "u2 = torch.zeros(X.size(dim=0),r)\n",
    "\n",
    "# Generating a random sample from a GP\n",
    "mean1 = gpytorch.means.ZeroMean()\n",
    "mean2 = gpytorch.means.ZeroMean()\n",
    "kernel1 = gpytorch.kernels.RBFKernel()\n",
    "kernel1.lengthscale = 1.5\n",
    "kernel2 = gpytorch.kernels.RBFKernel()\n",
    "kernel2.lengthscale = 0.2\n",
    "m1 = mean1(X)\n",
    "m2 = mean2(X)\n",
    "K1 = (kernel1(X,X) + kernel1(X,X_flipped) + kernel1(X_flipped,X) + kernel1(X_flipped,X_flipped)).add_jitter(1e-2).evaluate()\n",
    "K2 = (kernel2(X,X) + kernel2(X,X_flipped) + kernel2(X_flipped,X) + kernel2(X_flipped,X_flipped)).add_jitter(1e-2).evaluate()\n",
    "for i in range(r):\n",
    "    GP1 = gpytorch.distributions.MultivariateNormal(m1,K1)\n",
    "    GP2 = gpytorch.distributions.MultivariateNormal(m2,K2)\n",
    "    u1[:,i] = GP1.rsample().detach()\n",
    "    u2[:,i] = GP2.rsample().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bfe475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do plotting here so we don't generate a new sample everytime\n",
    "# These plots should look symmetric\n",
    "slice3 = 1\n",
    "slice4 = 0\n",
    "plt.contourf(x1_grid[:,:,slice3,slice4].numpy(),x2_grid[:,:,slice3,slice4].numpy(),u2[:,1].reshape(x1_grid.shape)[:,:,slice3,slice4])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating outputs as linear combinations of these latent functions\n",
    "# but now the lmc-parameters A is coming from another kernel over some covariates\n",
    "\n",
    "m = 10 # num_outputs\n",
    "covar_outputs1 = torch.cat([1+0.5*torch.randn(int(m/2.)),-1+0.5*torch.randn(int(m/2.))])\n",
    "covar_outputs2 = torch.linspace(-1,1,m)\n",
    "# These are just some really dumb covariates which will give some clustering structure\n",
    "covar_kernel1 = gpytorch.kernels.RBFKernel()\n",
    "covar_kernel1.lengthscale = 0.4\n",
    "covar_kernel1 = covar_kernel1(covar_outputs1)\n",
    "covar_kernel2 = gpytorch.kernels.PolynomialKernel(offset=0,power=2)\n",
    "covar_kernel2 = covar_kernel2(covar_outputs2)\n",
    "evals1, evecs1 = covar_kernel1.symeig(eigenvectors=True)\n",
    "evecs1 = evecs1[:, -r:]\n",
    "evals1 = linear_operator.operators.DiagLinearOperator(evals1[-r:])\n",
    "A1 = evecs1.matmul(evals1.sqrt()).evaluate().detach()\n",
    "\n",
    "evals2, evecs2 = covar_kernel2.symeig(eigenvectors=True)\n",
    "evecs2 = evecs2[:, -r:]\n",
    "evals2 = linear_operator.operators.DiagLinearOperator(evals2[-r:])\n",
    "A2 = evecs2.matmul(evals2.sqrt()).evaluate().detach()\n",
    "\n",
    "f = A1.matmul(u1.t()).t() + A2.matmul(u2.t()).t()\n",
    "#for i in range(m):\n",
    "#    plt.plot(x,f[:,i])\n",
    "    \n",
    "# Now generate some corresponding data\n",
    "sigma = 0.01\n",
    "y = f + sigma*torch.randn(f.size())\n",
    "\n",
    "# Now going to get it ready for minibatching\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Get everything into long format\n",
    "y_long = y.t().reshape(-1)\n",
    "x_long = X.repeat(m,1)\n",
    "task_indices = torch.arange(0,m).unsqueeze(1).repeat(1,n**4).reshape(-1).long()\n",
    "\n",
    "# For last task, will remove some central observations \n",
    "#y_long[-5000:] = float('nan')\n",
    "#x_long = x_long[~y_long.isnan()]\n",
    "#task_indices = task_indices[~y_long.isnan()]\n",
    "#y_long = y_long[~y_long.isnan()]\n",
    "\n",
    "train_dataset = TensorDataset(x_long, y_long,task_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=300, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce91384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do plotting here so we don't generate a new sample everytime\n",
    "# These plots should look symmetric\n",
    "slice3 = 0\n",
    "slice4 = 0\n",
    "plt.contourf(x1_grid[:,:,slice3,slice4].numpy(),x2_grid[:,:,slice3,slice4].numpy(),f[:,9].reshape(x1_grid.shape)[:,:,slice3,slice4])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14963a6",
   "metadata": {},
   "source": [
    "## Set up the individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synpred.variational.outputcovariance_lmc_variational_strategy import OutputCovarianceLMCVariationalStrategy\n",
    "from synpred.variational.permutation_invariant_variational_strategy import PermutationInvariantVariationalStrategy\n",
    "\n",
    "class ICM1(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self,output_covars):\n",
    "        print(\"Init is run!\")\n",
    "        permutation = torch.tensor([1,0,2,3])\n",
    "        # Different set of inducing points for each latent function\n",
    "        inducing_points = torch.rand(r,700,4)\n",
    "        # Same set of inducing points for each latent function\n",
    "        #inducing_points = torch.linspace(0,1,30).unsqueeze(-1).repeat(r,1,1)\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each latent\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([r])\n",
    "        )\n",
    "        #variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(\n",
    "        #    inducing_points.size(-2), batch_shape=torch.Size([r])\n",
    "        #)\n",
    "        \n",
    "        # Covariance over the outputs, and initialized by covariates\n",
    "        output_kernel = gpytorch.kernels.RBFKernel()\n",
    "        \n",
    "        # We have to wrap the VariationalStrategy in a ModifiedLMCVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = OutputCovarianceLMCVariationalStrategy(\n",
    "            PermutationInvariantVariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, \n",
    "                permutation=permutation,learn_inducing_locations=True\n",
    "            ),\n",
    "            output_kernel,\n",
    "            output_covars,\n",
    "            num_tasks=m,\n",
    "            num_latents=r,\n",
    "            latent_dim=-1\n",
    "        )\n",
    "        \n",
    "        super(ICM1,self).__init__(variational_strategy)\n",
    "        \n",
    "        # The mean and covariance modules could be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        # For the ICM model, we do not set them as batch as we want the same kernel for each latent\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af168b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synpred.variational.modified_lmc_variational_strategy import ModifiedLMCVariationalStrategy\n",
    "\n",
    "class ICM2(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self,output_covars):\n",
    "        print(\"Init is run!\")\n",
    "        # Different set of inducing points for each latent function\n",
    "        inducing_points = torch.rand(r,700,4)\n",
    "        permutation = torch.tensor([1,0,2,3])\n",
    "        # Same set of inducing points for each latent function\n",
    "        #inducing_points = torch.linspace(0,1,30).unsqueeze(-1).repeat(r,1,1)\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each latent\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([r])\n",
    "        )\n",
    "        #variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(\n",
    "        #    inducing_points.size(-2), batch_shape=torch.Size([r])\n",
    "        #)\n",
    "        \n",
    "        # Covariance over the outputs, and initialized by covariates\n",
    "        output_kernel = gpytorch.kernels.PolynomialKernel(offset=0,power=2)\n",
    "        \n",
    "        # We have to wrap the VariationalStrategy in a ModifiedLMCVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = OutputCovarianceLMCVariationalStrategy(\n",
    "            PermutationInvariantVariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, \n",
    "                permutation=permutation,learn_inducing_locations=True\n",
    "            ),\n",
    "            output_kernel,\n",
    "            output_covars,\n",
    "            num_tasks=m,\n",
    "            num_latents=r,\n",
    "            latent_dim=-1\n",
    "        )\n",
    "        \n",
    "        super(ICM2,self).__init__(variational_strategy)\n",
    "        \n",
    "        # The mean and covariance modules could be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        # For the ICM model, we do not set them as batch as we want the same kernel for each latent\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93debdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synpred.variational.sum_variational_strategy import SumVariationalStrategy\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "class LMC(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self,output_covars):\n",
    "        model1 = ICM1(covar_outputs1)\n",
    "        model2 = ICM2(covar_outputs2)\n",
    "        \n",
    "        # We have to wrap the VariationalStrategy in a ModifiedLMCVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = SumVariationalStrategy(ModuleList([model1,model2]))\n",
    "        \n",
    "        super(LMC,self).__init__(variational_strategy)\n",
    "        \n",
    "        # The mean and covariance modules could be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        # For the ICM model, we do not set them as batch as we want the same kernel for each latent\n",
    "        #self.mean_module1 = gpytorch.means.ZeroMean()\n",
    "        #self.covar_module1 = gpytorch.kernels.RBFKernel()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"is soemthing happening here?\")\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        #mean_x = self.mean_module(x)\n",
    "        #covar_x = self.covar_module(x)\n",
    "        #return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "model = LMC(covar_outputs1)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ddbe13",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "import tqdm.notebook\n",
    "num_epochs = 4 # Number of complete passes through the data\n",
    "\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "\n",
    "\n",
    "variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=y_long.size(0), lr=0.01)\n",
    "\n",
    "hyperparameter_optimizer = torch.optim.Adam([\n",
    "    {'params': model.hyperparameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=y_long.size(0))\n",
    "\n",
    "# We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "# effective for VI.\n",
    "epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    minibatch_iter = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    for x_batch, y_batch, task_batch in minibatch_iter:\n",
    "        variational_ngd_optimizer.zero_grad()\n",
    "        hyperparameter_optimizer.zero_grad()\n",
    "        output = model(x_batch,task_indices=task_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        variational_ngd_optimizer.step()\n",
    "        hyperparameter_optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97378e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.variational_strategy.models[0].covar_module.lengthscale)\n",
    "print(model.variational_strategy.models[1].covar_module.lengthscale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d10fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11\n",
    "x1_test = torch.linspace(0,1,n)\n",
    "x2_test = torch.linspace(0,1,n)\n",
    "x3_test = torch.linspace(0,1,n)\n",
    "x4_test = torch.linspace(0,1,n)\n",
    "x1_test_grid, x2_test_grid, x3_test_grid, x4_test_grid = torch.meshgrid(x1_test,x2_test,x3_test,x4_test,indexing='ij')\n",
    "X1_test = x1_test_grid.reshape(-1)\n",
    "X2_test = x2_test_grid.reshape(-1)\n",
    "X3_test = x3_test_grid.reshape(-1)\n",
    "X4_test = x4_test_grid.reshape(-1)\n",
    "X_test = torch.stack([X1_test,X2_test,X3_test,X4_test],dim=-1)\n",
    "X_test_flipped = torch.stack([X2_test,X1_test,X3_test,X4_test],dim=-1)\n",
    "test_indices = torch.arange(9,10).unsqueeze(1).repeat(1,n**4).reshape(-1).long() # Only predict 9th output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "# Make predictions\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #predictions = likelihood(model(test_x,task_indices=test_indices,task_covars=newoutput))\n",
    "    #predictions = likelihood(model(X_test,task_indices=test_indices))\n",
    "    predictions = likelihood(model(X_test,task_indices=test_indices))\n",
    "    mean = predictions.mean\n",
    "    lower, upper = predictions.confidence_region()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f06d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "# These plots should look symmetric\n",
    "slice3 = 2\n",
    "slice4 = 6\n",
    "plt.contourf(x1_test_grid[:,:,slice3,slice4].numpy(),x2_test_grid[:,:,slice3,slice4].numpy(),mean.reshape(x1_test_grid.shape)[:,:,slice3,slice4])\n",
    "plt.colorbar()\n",
    "#plt.xlim(0,1)\n",
    "#plt.ylim(0,1)\n",
    "#plt.plot(model.variational_strategy.inducing_points.detach()[:,0],model.variational_strategy.inducing_points.detach()[:,1],'k*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73699c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared with true function\n",
    "slice3 = 2\n",
    "slice4 = 6\n",
    "plt.contourf(x1_grid[:,:,slice3,slice4].numpy(),x2_grid[:,:,slice3,slice4].numpy(),f[:,9].reshape(x1_grid.shape)[:,:,slice3,slice4])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b062b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And look at the error surface\n",
    "slice3 = 2\n",
    "slice4 = 6\n",
    "error = mean.reshape(x1_test_grid.shape)[:,:,slice3,slice4] - f[:,9].reshape(x1_grid.shape)[:,:,slice3,slice4]\n",
    "plt.contourf(x1_grid[:,:,slice3,slice4].numpy(),x2_grid[:,:,slice3,slice4].numpy(),error)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = model.variational_strategy.models[0].variational_strategy.lmc_coefficients.detach().t()\n",
    "B = model.variational_strategy.models[1].variational_strategy.lmc_coefficients.detach().t()\n",
    "plt.matshow(A.matmul(A.t()))\n",
    "plt.matshow(B.matmul(B.t()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61584d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
